{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20i190005_IE684_Lab1__Ex2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 1. Exercise 2. }$\n",
        "\n",
        "Now we will consider a slightly different algorithm which can be used to find a minimizer of the function $f(\\mathbf{x})=f(x_1,x_2)= (x_1+100)^2 + (x_2-25)^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpe6eGRLvSh"
      },
      "source": [
        "$\\textbf{[R]}$ Write the function $f(\\mathbf{x})$ in the form $\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{x}\\in {\\mathbb{R}}^2$, $\\mathbf{A}$ is a symmetric matrix of size $2 \\times 2$, $\\mathbf{b}\\in{\\mathbb{R}}^2$ and $c\\in\\mathbb{R}$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPeLBt0L7F7"
      },
      "source": [
        "Write your answer here:\n",
        "\n",
        "\n",
        "\n",
        "Here, $A =\n",
        "\\begin{bmatrix}\n",
        "1 & 0\\\\\n",
        "0 & 1\n",
        "\\end{bmatrix},\n",
        "b = \\begin{bmatrix}\n",
        "-10\\\\\n",
        "2\n",
        "\\end{bmatrix}, c = 104$\n",
        "\n",
        "\n",
        "\n",
        "$\\therefore f(\\mathbf{x}) =  \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c =\\mathbf{x}^\\top \\begin{bmatrix}\n",
        "1 & 0\\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\\mathbf{x} + 2\\begin{bmatrix}\n",
        "-10\\\\\n",
        "2\n",
        "\\end{bmatrix}^\\top \\mathbf{x} + 104$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjANnIQ3L39D"
      },
      "source": [
        "\n",
        "$\\textbf{[R]}$ It turns out that for a function $f:{\\mathbb{R}}^n\\rightarrow \\mathbb{R}$ of the form $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{A}\\in{\\mathbb{R}}^{n \\times n}$ is a symmetric matrix, $\\mathbf{b} \\in {\\mathbb{R}}^n$ and $c\\in \\mathbb{R}$, the analytical solution to $\\min_{\\alpha \\geq 0} f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ can be found in closed form. Find the solution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jU-adJ0L-P1"
      },
      "source": [
        "Write your answer here:\n",
        "\n",
        "\n",
        "Let \n",
        "\\begin{align}\n",
        "g(\\alpha) &= f(x - \\alpha \\nabla f(x)) \\\\\n",
        "\\Rightarrow g'(\\alpha) &= (-\\nabla f(x)^\\top )(\\nabla f(x - \\alpha \\nabla f(x))) \\\\\n",
        "          &= (-\\nabla f(x)^\\top )(2 A(x - \\alpha \\nabla f(x)) + 2b) \\\\\n",
        "          &= -2(\\nabla f(x)^\\top A x - \\alpha \\nabla f(x)^\\top A \\nabla f(x) + \\nabla f(x)^\\top b) \\\\\n",
        "\\end{align}\n",
        "To find minima,  $g'(\\alpha) = 0$\n",
        "\\begin{align}\n",
        "\\Rightarrow \\alpha \\nabla f(x)^\\top A \\nabla f(x) &= \\nabla f(x)^\\top (A + 2b)\\\\\n",
        "\\Rightarrow \\alpha \\nabla f(x)^\\top A \\nabla f(x) &= \\frac{\\nabla f(x)^\\top \\nabla f(x)}{2} \\\\\n",
        "\\Rightarrow \\alpha &= \\frac{\\nabla f(x)^\\top \\nabla f(x)}{2\\nabla f(x)^\\top A \\nabla f(x)} \n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "We will use this idea to construct a suitable step length finding procedure for our modified algorithm given below: \n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "&\\textbf{While } \\| \\nabla f(\\mathbf{x}^k) \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} \\leftarrow \\mathbf{x}^k - \\eta^k \\nabla f(\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/1.19/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  return (x[0]+100)**2 + (x[1]-25)**2\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  return np.array([2*(x[0]+100),2*(x[1]-25)])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7vmaZy5dTjE"
      },
      "source": [
        "$\\textbf{Question 3}: $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength\n",
        "def compute_steplength(x): #add appropriate arguments to the function \n",
        "  #Complete the code \n",
        "  A = np.identity(2)\n",
        "  g = evalg(x)\n",
        "  g_t = np.matrix.transpose(g)\n",
        "  step_length = np.matmul(g_t, g)/(2*np.matmul(np.matmul(g_t, A), g))\n",
        "  return step_length"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    step_length = compute_steplength(x) #call the new function you wrote to compute the steplength\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x, k, evalf(x)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4047f759-3c28-4842-cb8c-e3975c6adf4c"
      },
      "source": [
        "my_start_x = np.array([10,10])\n",
        "my_tol= 1e-3\n",
        "find_minimizer(my_start_x, my_tol)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-100.,   25.]), 1, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZcs0yGpIwDV"
      },
      "source": [
        "$\\textbf{Question 4}: $\n",
        "\n",
        "With the starting point $x^0 = (10, 10)$ and the new module to compute $\\eta ^k$, we try $ \\eta = 10^{-p}$, where $p = 1, 2,\\cdots, 10.$\n",
        "For each $\\eta$ , we record the number of iterations taken by the algorithm to terminate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "253ca949-9d27-4cd2-f028-5410a702da57"
      },
      "source": [
        "my_start_x = np.array([10,10])\n",
        "iterations_counter = []\n",
        "tolerance_array = []\n",
        "for i in range(1,11):\n",
        "  my_tol= 10**-i\n",
        "  x, k, f_v = find_minimizer(my_start_x, my_tol)\n",
        "  iterations_counter.append(k)\n",
        "  tolerance_array.append(10**-i)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(tolerance_array, iterations_counter)\n",
        "plt.title(\"Graph of number of iterations against Tolerance values\")\n",
        "plt.xlabel(r'$\\tau$ values')\n",
        "plt.ylabel('Iterations')\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "toyyuSWbLcbj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7_NokutgczP"
      },
      "source": [
        "$f(\\mathbf{x})$ is a convex and quadratic function. Here in this exersize, we are using the exact line search because of that algorithm is directly converging to final minimizer within one loop. In the last exersize, we computed number of iterations for different step sizes, the ideal value of the step leangth was $0.5$ among the given values.\n",
        "\n",
        "Here plot is constant function where in the previous exersize we got different functions, we got more iterations to converge because of the fixed change in the step leangth. Here we use step length by finding the value of $\\min_{\\alpha \\geq 0} f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$. Since quaratic functions converges within one step if we use exact line search, algorithm is converging within one step irrespective of the initial point which was not the case in the last exersize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcR5z1jjgOru"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    }
  ]
}